{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffd1d728",
   "metadata": {},
   "source": [
    "# Policy Gradient on CartPole\n",
    "\n",
    "Simple REINFORCE implementation for CartPole balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60640d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54cdb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork:\n",
    "    def __init__(self, state_size=4, action_size=2, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.W1 = np.random.randn(state_size, 16) * 0.1\n",
    "        self.b1 = np.zeros(16)\n",
    "        self.W2 = np.random.randn(16, action_size) * 0.1\n",
    "        self.b2 = np.zeros(action_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        h = np.maximum(0, np.dot(state, self.W1) + self.b1)  # ReLU\n",
    "        logits = np.dot(h, self.W2) + self.b2\n",
    "        probs = self.softmax(logits)\n",
    "        return probs, h, logits\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        probs, _, _ = self.forward(state)\n",
    "        action = np.random.choice(len(probs), p=probs)\n",
    "        return action, probs\n",
    "    \n",
    "    def update(self, states, actions, rewards):\n",
    "        returns = self.calculate_returns(rewards)\n",
    "        \n",
    "        for state, action, G in zip(states, actions, returns):\n",
    "            probs, h, logits = self.forward(state)\n",
    "            \n",
    "            # Policy gradient\n",
    "            d_logits = probs.copy()\n",
    "            d_logits[action] -= 1\n",
    "            d_logits *= G\n",
    "            \n",
    "            # Backprop\n",
    "            dW2 = np.outer(h, d_logits)\n",
    "            db2 = d_logits\n",
    "            \n",
    "            dh = np.dot(d_logits, self.W2.T)\n",
    "            dh[h <= 0] = 0  # ReLU derivative\n",
    "            \n",
    "            dW1 = np.outer(state, dh)\n",
    "            db1 = dh\n",
    "            \n",
    "            # Update weights\n",
    "            self.W2 -= self.lr * dW2\n",
    "            self.b2 -= self.lr * db2\n",
    "            self.W1 -= self.lr * dW1\n",
    "            self.b1 -= self.lr * db1\n",
    "    \n",
    "    def calculate_returns(self, rewards, gamma=0.99):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(rewards):\n",
    "            G = reward + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_gradient(env, policy, episodes=1000):\n",
    "    scores = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        states, actions, rewards = [], [], []\n",
    "        \n",
    "        while True:\n",
    "            action, _ = policy.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        policy.update(states, actions, rewards)\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print(f\"Episode {episode}: Average Score = {avg_score:.1f}\")\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316531ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "env = gym.make('CartPole-v1')\n",
    "policy = PolicyNetwork(state_size=4, action_size=2, lr=0.01)\n",
    "\n",
    "scores = train_policy_gradient(env, policy, episodes=800)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(scores, alpha=0.6)\n",
    "window = 50\n",
    "moving_avg = [np.mean(scores[max(0, i-window):i+1]) for i in range(len(scores))]\n",
    "plt.plot(moving_avg, color='red', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Training Progress')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "recent_scores = scores[-100:]\n",
    "plt.hist(recent_scores, bins=20, alpha=0.7)\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Final 100 Episodes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final average score: {np.mean(scores[-100:]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dbb394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained policy\n",
    "def test_policy(env, policy, episodes=5):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while steps < 500:\n",
    "            action, probs = policy.get_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        print(f\"Test Episode {episode + 1}: Score = {total_reward}, Steps = {steps}\")\n",
    "\n",
    "test_policy(env, policy)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
